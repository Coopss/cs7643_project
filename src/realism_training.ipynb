{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"realism_training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"widgets":{"application/vnd.jupyter.widget-state+json":{"beab7ee6bc7c4636abd85be59764dbf4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_31d8f91a3b51406c99170185cb57bdf3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6c463fb29dff468ba9a4abfc69676b5c","IPY_MODEL_bfd46df70f74489b872f356189c75810"]}},"31d8f91a3b51406c99170185cb57bdf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c463fb29dff468ba9a4abfc69676b5c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_41a39bd7cf214487abae71f89dbbd7cf","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":411,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":411,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ceb872fcd75042fcb64813191ccfbcb2"}},"bfd46df70f74489b872f356189c75810":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_64832de14c284462995ef218b71a9142","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 411/411 [00:00&lt;00:00, 7.88kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb16ad835f9c48ae936c819fb9896d47"}},"41a39bd7cf214487abae71f89dbbd7cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ceb872fcd75042fcb64813191ccfbcb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"64832de14c284462995ef218b71a9142":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eb16ad835f9c48ae936c819fb9896d47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd2bec3cd6324680b3206b002e7ad447":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_65bf3f8663c34c53bd972ac6e9307251","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_37f8433ce9874701a6161a087aa3d1e9","IPY_MODEL_062a976ffa484e838443fa5325c09b11"]}},"65bf3f8663c34c53bd972ac6e9307251":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"37f8433ce9874701a6161a087aa3d1e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fd834db936584e5b8019ff84399e43c5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":263273408,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":263273408,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_67dbef804ef64af3a7945915b9f83db5"}},"062a976ffa484e838443fa5325c09b11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2ca9d49b34e14343865ab5f70949b5ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 263M/263M [00:06&lt;00:00, 38.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_40502d48f34a44308785142a13f8304a"}},"fd834db936584e5b8019ff84399e43c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"67dbef804ef64af3a7945915b9f83db5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ca9d49b34e14343865ab5f70949b5ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"40502d48f34a44308785142a13f8304a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luoxoSormhZ_","executionInfo":{"status":"ok","timestamp":1620063014333,"user_tz":240,"elapsed":913,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"f081596d-f8b5-4ddf-9230-d4797be3d284"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon May  3 17:30:13 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7Bv9E4XmhaD","executionInfo":{"status":"ok","timestamp":1620063099768,"user_tz":240,"elapsed":3548,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"df74ca46-ca3a-47a8-f799-f7a9ca7be27f"},"source":["import pickle\n","\n","import torch\n","!pip install transformers\n","from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from scipy import stats\n","import collections\n","import sklearn\n","import re\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2ItoPttklSj","executionInfo":{"status":"ok","timestamp":1620063121902,"user_tz":240,"elapsed":834,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"177e7674-dff9-4671-d552-4a0c507d3e92"},"source":["with open(\"/content/drive/My Drive/fakes_plus_real7000.pickle\", 'rb') as f:\n","    data = pickle.load(f)\n","\n","len(data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6902"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"7gbMORzFSPax","executionInfo":{"status":"ok","timestamp":1620063204834,"user_tz":240,"elapsed":586,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"29432c16-0cf1-4363-f9f6-50d9e4e433ee"},"source":["str = data[list(data.keys())[2]]['real_comment']\n","print(str)\n","str = re.sub(r'\\n', ' ', str)\n","str"],"execution_count":null,"outputs":[{"output_type":"stream","text":["i saw the new spiderman recently and i think it's the best spiderman film made so far. i also saw 21 and over like last week and it was pretty funny.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"i saw the new spiderman recently and i think it's the best spiderman film made so far. i also saw 21 and over like last week and it was pretty funny.\""]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"IUQhbJSymhaE"},"source":["# load sample data\n","comments = []\n","labels = []\n","for key in data.keys():\n","  real = data[key]['real_comment']\n","  if len(real) <= 500:\n","    comments.append(real)\n","    labels.append(1)\n","\n","  fake = data[key]['fake_comment']\n","  if len(fake) <= 500:\n","    comments.append(fake)\n","    labels.append(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewnC-nlZdQsp","executionInfo":{"status":"ok","timestamp":1620021823392,"user_tz":240,"elapsed":794,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"31c103df-a993-4818-8d41-16980d68263e"},"source":["len(comments)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13315"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_euJJvl7mhaF","executionInfo":{"status":"ok","timestamp":1620021823808,"user_tz":240,"elapsed":699,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"53331cae-6cbd-45a9-d789-6d35e62c58d4"},"source":["# split text\n","train_texts, test_texts, train_labels, test_labels = train_test_split(comments, labels, test_size=.1, train_size=.9, shuffle=True)\n","# train_texts, test_texts, train_labels, test_labels = train_test_split(posts_comms, upvotes, test_size=.1, train_size=.2, shuffle=True)\n","print(len(train_texts), len(train_labels), len(test_texts), len(test_labels))\n","\n","posts_comms = None"],"execution_count":null,"outputs":[{"output_type":"stream","text":["11983 11983 1332 1332\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dzk2XpipmhaF"},"source":["# load tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('distilbert-base-cased', use_cache=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsfdgwNgmhaF","executionInfo":{"status":"ok","timestamp":1620021835748,"user_tz":240,"elapsed":1630,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"c0da7cc0-e2de-4ff2-e415-04c40591e884"},"source":["# tokenize train data\n","train_encodings = tokenizer(text=train_texts, truncation=True, padding=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aZNIJhn-mhaG"},"source":["# tokenize test data\n","test_encodings = tokenizer(text=test_texts, truncation=True, padding=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["beab7ee6bc7c4636abd85be59764dbf4","31d8f91a3b51406c99170185cb57bdf3","6c463fb29dff468ba9a4abfc69676b5c","bfd46df70f74489b872f356189c75810","41a39bd7cf214487abae71f89dbbd7cf","ceb872fcd75042fcb64813191ccfbcb2","64832de14c284462995ef218b71a9142","eb16ad835f9c48ae936c819fb9896d47","dd2bec3cd6324680b3206b002e7ad447","65bf3f8663c34c53bd972ac6e9307251","37f8433ce9874701a6161a087aa3d1e9","062a976ffa484e838443fa5325c09b11","fd834db936584e5b8019ff84399e43c5","67dbef804ef64af3a7945915b9f83db5","2ca9d49b34e14343865ab5f70949b5ba","40502d48f34a44308785142a13f8304a"]},"id":"3Xq4Ue6MmhaH","executionInfo":{"status":"ok","timestamp":1620018919444,"user_tz":240,"elapsed":13627,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"b18b1cc2-a349-4680-e0e2-25e4ad818bd5"},"source":["# load model\n","model = BertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n","model.train()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"beab7ee6bc7c4636abd85be59764dbf4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd2bec3cd6324680b3206b002e7ad447","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"_UV-sbjWmhaH"},"source":["# create dataset class and load encodings and associated labels to it\n","\n","class upvote_prediction_dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = upvote_prediction_dataset(train_encodings, train_labels)\n","test_dataset = upvote_prediction_dataset(test_encodings, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BnALj5i2mhaI","executionInfo":{"status":"ok","timestamp":1620020605705,"user_tz":240,"elapsed":1698685,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"48dc835a-96ba-43c4-94a2-fa2dcc1dec5c"},"source":["training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=5,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=16,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=10,\n",")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=test_dataset             # evaluation dataset\n",")\n","\n","trainer.train()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='3745' max='3745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3745/3745 27:58, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.711500</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.699000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.688200</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.705300</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.674100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.693700</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.703500</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.678100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.690100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.639100</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.650800</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.671900</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.671800</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.629000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.607100</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.580300</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.636900</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.606000</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.541600</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.575400</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.611000</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.520500</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.550800</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.490500</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.576900</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.537300</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.681800</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.504300</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.567200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.540500</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.609300</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.492500</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.633700</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.788700</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.580500</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.870100</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.567000</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.635600</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.591200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.584200</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.572400</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.626700</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.665100</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.517800</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.716600</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.530800</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.462000</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.545400</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.447900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.575700</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.452500</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.578200</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.666900</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.472300</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.600800</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.508300</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.521200</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.592400</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>0.431100</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.437600</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>0.430600</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.457400</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.525700</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.543200</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.477200</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.605300</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>0.525000</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.527400</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.565200</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.478500</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>0.543700</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>0.433500</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>0.534400</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>0.517100</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.407200</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.561500</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>0.351600</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>0.462600</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>0.543200</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.498100</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>0.507200</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>0.516100</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>0.401300</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.445700</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.493900</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>0.405300</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>0.454000</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>0.449600</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>0.470100</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.381200</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>0.412200</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>0.457900</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>0.485700</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>0.486800</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.495400</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.386700</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>0.388100</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>0.457800</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>0.307700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.425000</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>0.412600</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>0.354100</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>0.466700</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>0.373400</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.518700</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>0.693200</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>0.578100</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>0.470900</td>\n","    </tr>\n","    <tr>\n","      <td>1090</td>\n","      <td>0.308400</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.437500</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>0.505000</td>\n","    </tr>\n","    <tr>\n","      <td>1120</td>\n","      <td>0.395400</td>\n","    </tr>\n","    <tr>\n","      <td>1130</td>\n","      <td>0.504900</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.517700</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.489500</td>\n","    </tr>\n","    <tr>\n","      <td>1160</td>\n","      <td>0.506900</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>0.410000</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>0.426600</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>0.377500</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.418300</td>\n","    </tr>\n","    <tr>\n","      <td>1210</td>\n","      <td>0.419900</td>\n","    </tr>\n","    <tr>\n","      <td>1220</td>\n","      <td>0.406700</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>0.378100</td>\n","    </tr>\n","    <tr>\n","      <td>1240</td>\n","      <td>0.407700</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.457300</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>0.416500</td>\n","    </tr>\n","    <tr>\n","      <td>1270</td>\n","      <td>0.348900</td>\n","    </tr>\n","    <tr>\n","      <td>1280</td>\n","      <td>0.440300</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>0.564700</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.414200</td>\n","    </tr>\n","    <tr>\n","      <td>1310</td>\n","      <td>0.401700</td>\n","    </tr>\n","    <tr>\n","      <td>1320</td>\n","      <td>0.404500</td>\n","    </tr>\n","    <tr>\n","      <td>1330</td>\n","      <td>0.376800</td>\n","    </tr>\n","    <tr>\n","      <td>1340</td>\n","      <td>0.426200</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.325200</td>\n","    </tr>\n","    <tr>\n","      <td>1360</td>\n","      <td>0.525500</td>\n","    </tr>\n","    <tr>\n","      <td>1370</td>\n","      <td>0.389100</td>\n","    </tr>\n","    <tr>\n","      <td>1380</td>\n","      <td>0.509700</td>\n","    </tr>\n","    <tr>\n","      <td>1390</td>\n","      <td>0.531100</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.386200</td>\n","    </tr>\n","    <tr>\n","      <td>1410</td>\n","      <td>0.404100</td>\n","    </tr>\n","    <tr>\n","      <td>1420</td>\n","      <td>0.427100</td>\n","    </tr>\n","    <tr>\n","      <td>1430</td>\n","      <td>0.415400</td>\n","    </tr>\n","    <tr>\n","      <td>1440</td>\n","      <td>0.393200</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>0.363400</td>\n","    </tr>\n","    <tr>\n","      <td>1460</td>\n","      <td>0.513600</td>\n","    </tr>\n","    <tr>\n","      <td>1470</td>\n","      <td>0.466100</td>\n","    </tr>\n","    <tr>\n","      <td>1480</td>\n","      <td>0.409900</td>\n","    </tr>\n","    <tr>\n","      <td>1490</td>\n","      <td>0.338000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.482700</td>\n","    </tr>\n","    <tr>\n","      <td>1510</td>\n","      <td>0.322700</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.240000</td>\n","    </tr>\n","    <tr>\n","      <td>1530</td>\n","      <td>0.435600</td>\n","    </tr>\n","    <tr>\n","      <td>1540</td>\n","      <td>0.349200</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>0.332000</td>\n","    </tr>\n","    <tr>\n","      <td>1560</td>\n","      <td>0.490600</td>\n","    </tr>\n","    <tr>\n","      <td>1570</td>\n","      <td>0.264500</td>\n","    </tr>\n","    <tr>\n","      <td>1580</td>\n","      <td>0.319000</td>\n","    </tr>\n","    <tr>\n","      <td>1590</td>\n","      <td>0.360100</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.400600</td>\n","    </tr>\n","    <tr>\n","      <td>1610</td>\n","      <td>0.327700</td>\n","    </tr>\n","    <tr>\n","      <td>1620</td>\n","      <td>0.278900</td>\n","    </tr>\n","    <tr>\n","      <td>1630</td>\n","      <td>0.287500</td>\n","    </tr>\n","    <tr>\n","      <td>1640</td>\n","      <td>0.372400</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>0.278800</td>\n","    </tr>\n","    <tr>\n","      <td>1660</td>\n","      <td>0.353700</td>\n","    </tr>\n","    <tr>\n","      <td>1670</td>\n","      <td>0.245500</td>\n","    </tr>\n","    <tr>\n","      <td>1680</td>\n","      <td>0.310600</td>\n","    </tr>\n","    <tr>\n","      <td>1690</td>\n","      <td>0.351400</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.283400</td>\n","    </tr>\n","    <tr>\n","      <td>1710</td>\n","      <td>0.320400</td>\n","    </tr>\n","    <tr>\n","      <td>1720</td>\n","      <td>0.299100</td>\n","    </tr>\n","    <tr>\n","      <td>1730</td>\n","      <td>0.253700</td>\n","    </tr>\n","    <tr>\n","      <td>1740</td>\n","      <td>0.320800</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.293400</td>\n","    </tr>\n","    <tr>\n","      <td>1760</td>\n","      <td>0.336300</td>\n","    </tr>\n","    <tr>\n","      <td>1770</td>\n","      <td>0.289100</td>\n","    </tr>\n","    <tr>\n","      <td>1780</td>\n","      <td>0.324400</td>\n","    </tr>\n","    <tr>\n","      <td>1790</td>\n","      <td>0.357300</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.260000</td>\n","    </tr>\n","    <tr>\n","      <td>1810</td>\n","      <td>0.380300</td>\n","    </tr>\n","    <tr>\n","      <td>1820</td>\n","      <td>0.360800</td>\n","    </tr>\n","    <tr>\n","      <td>1830</td>\n","      <td>0.390400</td>\n","    </tr>\n","    <tr>\n","      <td>1840</td>\n","      <td>0.259900</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>0.318700</td>\n","    </tr>\n","    <tr>\n","      <td>1860</td>\n","      <td>0.289400</td>\n","    </tr>\n","    <tr>\n","      <td>1870</td>\n","      <td>0.310900</td>\n","    </tr>\n","    <tr>\n","      <td>1880</td>\n","      <td>0.504600</td>\n","    </tr>\n","    <tr>\n","      <td>1890</td>\n","      <td>0.262900</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.386100</td>\n","    </tr>\n","    <tr>\n","      <td>1910</td>\n","      <td>0.293600</td>\n","    </tr>\n","    <tr>\n","      <td>1920</td>\n","      <td>0.328600</td>\n","    </tr>\n","    <tr>\n","      <td>1930</td>\n","      <td>0.279600</td>\n","    </tr>\n","    <tr>\n","      <td>1940</td>\n","      <td>0.341600</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>0.395000</td>\n","    </tr>\n","    <tr>\n","      <td>1960</td>\n","      <td>0.301700</td>\n","    </tr>\n","    <tr>\n","      <td>1970</td>\n","      <td>0.278000</td>\n","    </tr>\n","    <tr>\n","      <td>1980</td>\n","      <td>0.364800</td>\n","    </tr>\n","    <tr>\n","      <td>1990</td>\n","      <td>0.362400</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.285600</td>\n","    </tr>\n","    <tr>\n","      <td>2010</td>\n","      <td>0.301900</td>\n","    </tr>\n","    <tr>\n","      <td>2020</td>\n","      <td>0.250000</td>\n","    </tr>\n","    <tr>\n","      <td>2030</td>\n","      <td>0.220800</td>\n","    </tr>\n","    <tr>\n","      <td>2040</td>\n","      <td>0.359400</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>0.381300</td>\n","    </tr>\n","    <tr>\n","      <td>2060</td>\n","      <td>0.440300</td>\n","    </tr>\n","    <tr>\n","      <td>2070</td>\n","      <td>0.273300</td>\n","    </tr>\n","    <tr>\n","      <td>2080</td>\n","      <td>0.257200</td>\n","    </tr>\n","    <tr>\n","      <td>2090</td>\n","      <td>0.377800</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.296300</td>\n","    </tr>\n","    <tr>\n","      <td>2110</td>\n","      <td>0.353500</td>\n","    </tr>\n","    <tr>\n","      <td>2120</td>\n","      <td>0.276700</td>\n","    </tr>\n","    <tr>\n","      <td>2130</td>\n","      <td>0.295700</td>\n","    </tr>\n","    <tr>\n","      <td>2140</td>\n","      <td>0.341300</td>\n","    </tr>\n","    <tr>\n","      <td>2150</td>\n","      <td>0.459900</td>\n","    </tr>\n","    <tr>\n","      <td>2160</td>\n","      <td>0.363200</td>\n","    </tr>\n","    <tr>\n","      <td>2170</td>\n","      <td>0.422700</td>\n","    </tr>\n","    <tr>\n","      <td>2180</td>\n","      <td>0.262500</td>\n","    </tr>\n","    <tr>\n","      <td>2190</td>\n","      <td>0.274500</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.317300</td>\n","    </tr>\n","    <tr>\n","      <td>2210</td>\n","      <td>0.265200</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>0.241900</td>\n","    </tr>\n","    <tr>\n","      <td>2230</td>\n","      <td>0.311400</td>\n","    </tr>\n","    <tr>\n","      <td>2240</td>\n","      <td>0.315700</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.426500</td>\n","    </tr>\n","    <tr>\n","      <td>2260</td>\n","      <td>0.218400</td>\n","    </tr>\n","    <tr>\n","      <td>2270</td>\n","      <td>0.272600</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.314100</td>\n","    </tr>\n","    <tr>\n","      <td>2290</td>\n","      <td>0.259500</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.363300</td>\n","    </tr>\n","    <tr>\n","      <td>2310</td>\n","      <td>0.251600</td>\n","    </tr>\n","    <tr>\n","      <td>2320</td>\n","      <td>0.225200</td>\n","    </tr>\n","    <tr>\n","      <td>2330</td>\n","      <td>0.226700</td>\n","    </tr>\n","    <tr>\n","      <td>2340</td>\n","      <td>0.253300</td>\n","    </tr>\n","    <tr>\n","      <td>2350</td>\n","      <td>0.230400</td>\n","    </tr>\n","    <tr>\n","      <td>2360</td>\n","      <td>0.254900</td>\n","    </tr>\n","    <tr>\n","      <td>2370</td>\n","      <td>0.255100</td>\n","    </tr>\n","    <tr>\n","      <td>2380</td>\n","      <td>0.228500</td>\n","    </tr>\n","    <tr>\n","      <td>2390</td>\n","      <td>0.197400</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.339100</td>\n","    </tr>\n","    <tr>\n","      <td>2410</td>\n","      <td>0.239100</td>\n","    </tr>\n","    <tr>\n","      <td>2420</td>\n","      <td>0.289700</td>\n","    </tr>\n","    <tr>\n","      <td>2430</td>\n","      <td>0.275700</td>\n","    </tr>\n","    <tr>\n","      <td>2440</td>\n","      <td>0.356900</td>\n","    </tr>\n","    <tr>\n","      <td>2450</td>\n","      <td>0.231500</td>\n","    </tr>\n","    <tr>\n","      <td>2460</td>\n","      <td>0.246300</td>\n","    </tr>\n","    <tr>\n","      <td>2470</td>\n","      <td>0.163500</td>\n","    </tr>\n","    <tr>\n","      <td>2480</td>\n","      <td>0.247900</td>\n","    </tr>\n","    <tr>\n","      <td>2490</td>\n","      <td>0.270800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.265400</td>\n","    </tr>\n","    <tr>\n","      <td>2510</td>\n","      <td>0.142000</td>\n","    </tr>\n","    <tr>\n","      <td>2520</td>\n","      <td>0.341200</td>\n","    </tr>\n","    <tr>\n","      <td>2530</td>\n","      <td>0.218000</td>\n","    </tr>\n","    <tr>\n","      <td>2540</td>\n","      <td>0.342100</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>0.196900</td>\n","    </tr>\n","    <tr>\n","      <td>2560</td>\n","      <td>0.393800</td>\n","    </tr>\n","    <tr>\n","      <td>2570</td>\n","      <td>0.362900</td>\n","    </tr>\n","    <tr>\n","      <td>2580</td>\n","      <td>0.218400</td>\n","    </tr>\n","    <tr>\n","      <td>2590</td>\n","      <td>0.256600</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.151800</td>\n","    </tr>\n","    <tr>\n","      <td>2610</td>\n","      <td>0.302300</td>\n","    </tr>\n","    <tr>\n","      <td>2620</td>\n","      <td>0.242500</td>\n","    </tr>\n","    <tr>\n","      <td>2630</td>\n","      <td>0.259300</td>\n","    </tr>\n","    <tr>\n","      <td>2640</td>\n","      <td>0.151900</td>\n","    </tr>\n","    <tr>\n","      <td>2650</td>\n","      <td>0.358800</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.218600</td>\n","    </tr>\n","    <tr>\n","      <td>2670</td>\n","      <td>0.194800</td>\n","    </tr>\n","    <tr>\n","      <td>2680</td>\n","      <td>0.212400</td>\n","    </tr>\n","    <tr>\n","      <td>2690</td>\n","      <td>0.324900</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.309600</td>\n","    </tr>\n","    <tr>\n","      <td>2710</td>\n","      <td>0.194500</td>\n","    </tr>\n","    <tr>\n","      <td>2720</td>\n","      <td>0.285900</td>\n","    </tr>\n","    <tr>\n","      <td>2730</td>\n","      <td>0.410300</td>\n","    </tr>\n","    <tr>\n","      <td>2740</td>\n","      <td>0.249200</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.251800</td>\n","    </tr>\n","    <tr>\n","      <td>2760</td>\n","      <td>0.175300</td>\n","    </tr>\n","    <tr>\n","      <td>2770</td>\n","      <td>0.248600</td>\n","    </tr>\n","    <tr>\n","      <td>2780</td>\n","      <td>0.196000</td>\n","    </tr>\n","    <tr>\n","      <td>2790</td>\n","      <td>0.258900</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.303400</td>\n","    </tr>\n","    <tr>\n","      <td>2810</td>\n","      <td>0.245600</td>\n","    </tr>\n","    <tr>\n","      <td>2820</td>\n","      <td>0.266400</td>\n","    </tr>\n","    <tr>\n","      <td>2830</td>\n","      <td>0.358200</td>\n","    </tr>\n","    <tr>\n","      <td>2840</td>\n","      <td>0.338300</td>\n","    </tr>\n","    <tr>\n","      <td>2850</td>\n","      <td>0.242600</td>\n","    </tr>\n","    <tr>\n","      <td>2860</td>\n","      <td>0.259600</td>\n","    </tr>\n","    <tr>\n","      <td>2870</td>\n","      <td>0.234200</td>\n","    </tr>\n","    <tr>\n","      <td>2880</td>\n","      <td>0.257300</td>\n","    </tr>\n","    <tr>\n","      <td>2890</td>\n","      <td>0.316900</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.178100</td>\n","    </tr>\n","    <tr>\n","      <td>2910</td>\n","      <td>0.221500</td>\n","    </tr>\n","    <tr>\n","      <td>2920</td>\n","      <td>0.192100</td>\n","    </tr>\n","    <tr>\n","      <td>2930</td>\n","      <td>0.198700</td>\n","    </tr>\n","    <tr>\n","      <td>2940</td>\n","      <td>0.262800</td>\n","    </tr>\n","    <tr>\n","      <td>2950</td>\n","      <td>0.196900</td>\n","    </tr>\n","    <tr>\n","      <td>2960</td>\n","      <td>0.183200</td>\n","    </tr>\n","    <tr>\n","      <td>2970</td>\n","      <td>0.277400</td>\n","    </tr>\n","    <tr>\n","      <td>2980</td>\n","      <td>0.203500</td>\n","    </tr>\n","    <tr>\n","      <td>2990</td>\n","      <td>0.191000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.233300</td>\n","    </tr>\n","    <tr>\n","      <td>3010</td>\n","      <td>0.107100</td>\n","    </tr>\n","    <tr>\n","      <td>3020</td>\n","      <td>0.215500</td>\n","    </tr>\n","    <tr>\n","      <td>3030</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.260100</td>\n","    </tr>\n","    <tr>\n","      <td>3050</td>\n","      <td>0.245800</td>\n","    </tr>\n","    <tr>\n","      <td>3060</td>\n","      <td>0.172000</td>\n","    </tr>\n","    <tr>\n","      <td>3070</td>\n","      <td>0.185700</td>\n","    </tr>\n","    <tr>\n","      <td>3080</td>\n","      <td>0.148500</td>\n","    </tr>\n","    <tr>\n","      <td>3090</td>\n","      <td>0.213900</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.208000</td>\n","    </tr>\n","    <tr>\n","      <td>3110</td>\n","      <td>0.185100</td>\n","    </tr>\n","    <tr>\n","      <td>3120</td>\n","      <td>0.228800</td>\n","    </tr>\n","    <tr>\n","      <td>3130</td>\n","      <td>0.347700</td>\n","    </tr>\n","    <tr>\n","      <td>3140</td>\n","      <td>0.190900</td>\n","    </tr>\n","    <tr>\n","      <td>3150</td>\n","      <td>0.160400</td>\n","    </tr>\n","    <tr>\n","      <td>3160</td>\n","      <td>0.346100</td>\n","    </tr>\n","    <tr>\n","      <td>3170</td>\n","      <td>0.220300</td>\n","    </tr>\n","    <tr>\n","      <td>3180</td>\n","      <td>0.261300</td>\n","    </tr>\n","    <tr>\n","      <td>3190</td>\n","      <td>0.153000</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.191500</td>\n","    </tr>\n","    <tr>\n","      <td>3210</td>\n","      <td>0.259700</td>\n","    </tr>\n","    <tr>\n","      <td>3220</td>\n","      <td>0.346100</td>\n","    </tr>\n","    <tr>\n","      <td>3230</td>\n","      <td>0.234800</td>\n","    </tr>\n","    <tr>\n","      <td>3240</td>\n","      <td>0.143400</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.249700</td>\n","    </tr>\n","    <tr>\n","      <td>3260</td>\n","      <td>0.085300</td>\n","    </tr>\n","    <tr>\n","      <td>3270</td>\n","      <td>0.084300</td>\n","    </tr>\n","    <tr>\n","      <td>3280</td>\n","      <td>0.240200</td>\n","    </tr>\n","    <tr>\n","      <td>3290</td>\n","      <td>0.143400</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.342700</td>\n","    </tr>\n","    <tr>\n","      <td>3310</td>\n","      <td>0.191800</td>\n","    </tr>\n","    <tr>\n","      <td>3320</td>\n","      <td>0.279200</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>0.094200</td>\n","    </tr>\n","    <tr>\n","      <td>3340</td>\n","      <td>0.379500</td>\n","    </tr>\n","    <tr>\n","      <td>3350</td>\n","      <td>0.214400</td>\n","    </tr>\n","    <tr>\n","      <td>3360</td>\n","      <td>0.178300</td>\n","    </tr>\n","    <tr>\n","      <td>3370</td>\n","      <td>0.165100</td>\n","    </tr>\n","    <tr>\n","      <td>3380</td>\n","      <td>0.248800</td>\n","    </tr>\n","    <tr>\n","      <td>3390</td>\n","      <td>0.134100</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.170400</td>\n","    </tr>\n","    <tr>\n","      <td>3410</td>\n","      <td>0.344400</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.161500</td>\n","    </tr>\n","    <tr>\n","      <td>3430</td>\n","      <td>0.153200</td>\n","    </tr>\n","    <tr>\n","      <td>3440</td>\n","      <td>0.201700</td>\n","    </tr>\n","    <tr>\n","      <td>3450</td>\n","      <td>0.152400</td>\n","    </tr>\n","    <tr>\n","      <td>3460</td>\n","      <td>0.147300</td>\n","    </tr>\n","    <tr>\n","      <td>3470</td>\n","      <td>0.354000</td>\n","    </tr>\n","    <tr>\n","      <td>3480</td>\n","      <td>0.159100</td>\n","    </tr>\n","    <tr>\n","      <td>3490</td>\n","      <td>0.200900</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.244100</td>\n","    </tr>\n","    <tr>\n","      <td>3510</td>\n","      <td>0.249100</td>\n","    </tr>\n","    <tr>\n","      <td>3520</td>\n","      <td>0.178900</td>\n","    </tr>\n","    <tr>\n","      <td>3530</td>\n","      <td>0.135800</td>\n","    </tr>\n","    <tr>\n","      <td>3540</td>\n","      <td>0.166700</td>\n","    </tr>\n","    <tr>\n","      <td>3550</td>\n","      <td>0.206800</td>\n","    </tr>\n","    <tr>\n","      <td>3560</td>\n","      <td>0.171200</td>\n","    </tr>\n","    <tr>\n","      <td>3570</td>\n","      <td>0.209500</td>\n","    </tr>\n","    <tr>\n","      <td>3580</td>\n","      <td>0.167400</td>\n","    </tr>\n","    <tr>\n","      <td>3590</td>\n","      <td>0.200800</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.191500</td>\n","    </tr>\n","    <tr>\n","      <td>3610</td>\n","      <td>0.135800</td>\n","    </tr>\n","    <tr>\n","      <td>3620</td>\n","      <td>0.155600</td>\n","    </tr>\n","    <tr>\n","      <td>3630</td>\n","      <td>0.178900</td>\n","    </tr>\n","    <tr>\n","      <td>3640</td>\n","      <td>0.248300</td>\n","    </tr>\n","    <tr>\n","      <td>3650</td>\n","      <td>0.283600</td>\n","    </tr>\n","    <tr>\n","      <td>3660</td>\n","      <td>0.221300</td>\n","    </tr>\n","    <tr>\n","      <td>3670</td>\n","      <td>0.123700</td>\n","    </tr>\n","    <tr>\n","      <td>3680</td>\n","      <td>0.202600</td>\n","    </tr>\n","    <tr>\n","      <td>3690</td>\n","      <td>0.140300</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.184600</td>\n","    </tr>\n","    <tr>\n","      <td>3710</td>\n","      <td>0.195600</td>\n","    </tr>\n","    <tr>\n","      <td>3720</td>\n","      <td>0.138400</td>\n","    </tr>\n","    <tr>\n","      <td>3730</td>\n","      <td>0.235600</td>\n","    </tr>\n","    <tr>\n","      <td>3740</td>\n","      <td>0.149200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=3745, training_loss=0.36139661296505793, metrics={'train_runtime': 1678.8893, 'train_samples_per_second': 2.231, 'total_flos': 8059961603418300.0, 'epoch': 5.0, 'init_mem_cpu_alloc_delta': 1874640896, 'init_mem_gpu_alloc_delta': 433776128, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 17035264, 'train_mem_gpu_alloc_delta': 1321523712, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 3455978496})"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"Huwqu4Z46NCa"},"source":["trainer.save_model('/content/drive/My Drive/realism_discriminator')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYVC9NoT0kJ8"},"source":["def batch(iterable, n=1):\n","    l = len(iterable)\n","    for ndx in range(0, l, n):\n","        yield iterable[ndx:min(ndx + n, l)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gaxWH-cGzzRf","executionInfo":{"status":"ok","timestamp":1620021808113,"user_tz":240,"elapsed":567,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"7171642d-0396-4340-b1ff-179770f67ee6"},"source":["# Check device availability\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')\n","print(\"You are using device: %s\" % device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["You are using device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8HPRdOYolgij"},"source":["# load model\n","model = BertForSequenceClassification.from_pretrained('/content/drive/My Drive/realism_discriminator', num_labels=2)\n","model.eval()\n","\n","model.to(device)\n","items = {key: torch.tensor(val).to(device) for key, val in test_encodings.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmzYP7lxx1If"},"source":["preds = []\n","for batch_idxs in batch(range(len(items['input_ids'])), 16):\n","  batch_input_ids = items['input_ids'][batch_idxs]\n","  batch_attention_mask = items['attention_mask'][batch_idxs]\n","  batch_token_type_ids = items['token_type_ids'][batch_idxs]\n","\n","  outs = model.forward(input_ids=batch_input_ids, attention_mask=batch_attention_mask, token_type_ids=batch_token_type_ids)\n","  probs = torch.softmax(outs.logits, 1)\n","  classes = torch.argmax(probs, dim=1)\n","\n","  preds += list(classes.cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJVUM3No0svv","executionInfo":{"status":"ok","timestamp":1620022463735,"user_tz":240,"elapsed":650,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"f3c6f651-4edc-4a8f-826b-c7d60fa5695e"},"source":["sklearn.metrics.f1_score(preds, test_labels, average='weighted')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9421676109984425"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3YsUeqD24wh","executionInfo":{"status":"ok","timestamp":1620022525361,"user_tz":240,"elapsed":802,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"397a8a9c-e1e8-4cac-8840-ecc11bd5ec96"},"source":["sklearn.metrics.confusion_matrix(preds, test_labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[702,  36],\n","       [ 41, 553]])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"Z8dmmmDz3fpN"},"source":[""],"execution_count":null,"outputs":[]}]}